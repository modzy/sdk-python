{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import shutil\n",
    "import json\n",
    "import os\n",
    "import ntpath\n",
    "import tarfile\n",
    "from libcloud.storage.types import Provider\n",
    "from libcloud.storage.providers import get_driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_resources(model_yaml_path,container,resources_key,\n",
    "                     storage_key,storage_secret,storage_provider,additional_filepaths=None):\n",
    "    \"\"\" Creates resources archive expected by model converter, uploads to storage provider.\n",
    "    Args:\n",
    "        model_yaml_path (str): Path to model.yaml file to be included.\n",
    "        container (str): Storage provider container name (e.g. Bucket name in S3).\n",
    "        resources_key (str): Desired key for resource archive once uploaded to storage provider.\n",
    "        storage_key (str): Storage provider access key.\n",
    "        storage_secret (str): Storage provider secret key.\n",
    "        storage_provider (str): Storage provider name (must be one of \"S3\", \"AZURE_BLOBS\", or \"GOOGLE_STORAGE\").\n",
    "        additional_filepaths (list): List of filepaths of additional files to be included.\n",
    "    \"\"\"\n",
    "    # Init libcloud driver\n",
    "    if storage_provider == \"S3\":\n",
    "        cls = get_driver(Provider.S3)\n",
    "    elif storage_provider == \"AZURE_BLOBS\":\n",
    "        cls = get_driver(Provider.AZURE_BLOBS)\n",
    "    elif storage_provider == \"GOOGLE_STORAGE\":\n",
    "        cls = get_driver(Provider.GOOGLE_STORAGE)\n",
    "    else:\n",
    "        raise ValueError('Only \"S3\", \"AZURE_BLOBS\", and \"GOOGLE_STORAGE\" are supported storage providers.')\n",
    "        \n",
    "    driver = cls(storage_key, storage_secret)\n",
    "    container = driver.get_container(container_name=container)\n",
    "    \n",
    "    # TODO: Probably set these outside of this helper function\n",
    "    RESOURCES_TAR_NAME = \"resources.tar.gz\"\n",
    "    MODEL_YAML_NAME = \"model.yaml\"\n",
    "    \n",
    "    # Create temp dir\n",
    "    tmp_dir_path = os.path.join(os.getcwd(),\".tmp_\"+str(time.time()))\n",
    "    os.mkdir(tmp_dir_path)\n",
    "        \n",
    "    # Move the local resources that you have prepared for your model into an archive\n",
    "    resources_tar_path = os.path.join(tmp_dir_path,RESOURCES_TAR_NAME)\n",
    "    tar = tarfile.open(resources_tar_path, \"w:gz\")\n",
    "    tar.add(model_yaml_path,arcname=MODEL_YAML_NAME)\n",
    "    for filepath in additional_filepaths:\n",
    "        tar.add(filepath,arcname=ntpath.split(filepath)[1])\n",
    "    tar.close()\n",
    "\n",
    "    # This method blocks until all the parts have been uploaded.\n",
    "    extra = {'content_type': 'application/octet-stream'}\n",
    "    \n",
    "    # Upload archive to storage provider\n",
    "    with open(resources_tar_path, 'rb') as iterator:\n",
    "        obj = driver.upload_object_via_stream(iterator=iterator,\n",
    "                                              container=container,\n",
    "                                              object_name=resources_key,\n",
    "                                              extra=extra)\n",
    "        \n",
    "    # Remove temp dir\n",
    "    shutil.rmtree(tmp_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_mlflow_model(mlflow_model_dir,container,model_key,\n",
    "                     storage_key,storage_secret,storage_provider):\n",
    "    \"\"\" Creates resources archive expected by model converter, uploads to storage provider.\n",
    "    Args:\n",
    "        mlflow_model_dir (str): Path to saved MLFlow model directory (e.g. using mlflow.sklearn.save_model())\n",
    "        container (str): Storage provider container name (e.g. Bucket name in S3).\n",
    "        resources_key (str): Desired key for model archive once uploaded to storage provider.\n",
    "        storage_key (str): Storage provider access key.\n",
    "        storage_secret (str): Storage provider secret key.\n",
    "        storage_provider (str): Storage provider name (must be one of \"S3\", \"AZURE_BLOBS\", or \"GOOGLE_STORAGE\").\n",
    "    \"\"\"\n",
    "    # Init libcloud driver\n",
    "    if storage_provider == \"S3\":\n",
    "        cls = get_driver(Provider.S3)\n",
    "    elif storage_provider == \"AZURE_BLOBS\":\n",
    "        cls = get_driver(Provider.AZURE_BLOBS)\n",
    "    elif storage_provider == \"GOOGLE_STORAGE\":\n",
    "        cls = get_driver(Provider.GOOGLE_STORAGE)\n",
    "    else:\n",
    "        raise ValueError('Only \"S3\", \"AZURE_BLOBS\", and \"GOOGLE_STORAGE\" are supported storage providers.')\n",
    "        \n",
    "    driver = cls(storage_key, storage_secret)\n",
    "    container = driver.get_container(container_name=container)\n",
    "    \n",
    "    # TODO: Probably set this outside of this helper function\n",
    "    MODEL_TAR_NAME = \"weights.tar.gz\"\n",
    "    \n",
    "    # Create temp dir\n",
    "    tmp_dir_path = os.path.join(os.getcwd(),\".tmp_\"+str(time.time()))\n",
    "    os.mkdir(tmp_dir_path)\n",
    "        \n",
    "    # Move the local mlflow model artifacts that were saved out by MLFlow into an archive\n",
    "    model_tar_path = os.path.join(tmp_dir_path,MODEL_TAR_NAME)\n",
    "    tar = tarfile.open(model_tar_path, \"w:gz\")\n",
    "    mlflow_model_filenames = os.listdir(mlflow_model_dir)\n",
    "    for filename in mlflow_model_filenames:\n",
    "        full_path = os.path.join(mlflow_model_dir,filename)\n",
    "        tar.add(full_path,arcname=filename)\n",
    "    tar.close()\n",
    "\n",
    "    # This method blocks until all the parts have been uploaded.\n",
    "    extra = {'content_type': 'application/octet-stream'}\n",
    "    \n",
    "    # Upload archive to storage provider\n",
    "    with open(model_tar_path, 'rb') as iterator:\n",
    "        obj = driver.upload_object_via_stream(iterator=iterator,\n",
    "                                              container=container,\n",
    "                                              object_name=model_key,\n",
    "                                              extra=extra)\n",
    "        \n",
    "    # Remove temp dir\n",
    "    shutil.rmtree(tmp_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set general params\n",
    "BUCKET_NAME = \"sagemaker-testing-ds\"\n",
    "STORAGE_PROVIDER = \"S3\"\n",
    "storage_key = \"ACCESS_KEY_HERE\"\n",
    "storage_secret = \"SECRET_KEY_HERE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set resources-specific params\n",
    "RESOURCES_KEY = \"helper-testing/resources.tar.gz\"\n",
    "model_yaml_path = \"/path/to/model.yaml\"\n",
    "additional_filepaths = [\"/path/to/labels.json\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload resources archive to S3\n",
    "upload_resources(model_yaml_path,BUCKET_NAME,RESOURCES_KEY,\n",
    "                                storage_key,storage_secret,STORAGE_PROVIDER,additional_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set mlflow-specific params\n",
    "MODEL_KEY = \"helper-testing/weights.tar.gz\"\n",
    "mlflow_dir = \"/path/to/mlflow_saved_model_dir/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload mlflow model archive to S3\n",
    "upload_mlflow_model(mlflow_dir,BUCKET_NAME,MODEL_KEY,\n",
    "                                storage_key,storage_secret,STORAGE_PROVIDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
